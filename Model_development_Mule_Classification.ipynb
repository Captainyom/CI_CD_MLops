{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[486   0]\n",
      " [  0 514]]\n",
      "-----------------------------------\n",
      "SVM Model:\n",
      "Accuracy: 0.804\n",
      "Precision: 0.740909090909091\n",
      "Recall: 0.9513618677042801\n",
      "F1 Score: 0.8330494037478705\n",
      "Confusion Matrix:\n",
      "[[315 171]\n",
      " [ 25 489]]\n",
      "-----------------------------------\n",
      "Gradient Boosting Model:\n",
      "Accuracy: 0.999\n",
      "Precision: 0.9980582524271845\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9990281827016522\n",
      "Confusion Matrix:\n",
      "[[485   1]\n",
      " [  0 514]]\n",
      "-----------------------------------\n",
      "Decision Tree Model:\n",
      "Accuracy: 0.999\n",
      "Precision: 0.9980582524271845\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9990281827016522\n",
      "Confusion Matrix:\n",
      "[[485   1]\n",
      " [  0 514]]\n",
      "-----------------------------------\n",
      "Random Forest Model:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[486   0]\n",
      " [  0 514]]\n",
      "-----------------------------------\n",
      "Best Model: XGBoost with F1 Score: 1.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1992\\3034754846.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    499\u001b[0m }\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Function to generate data with specified statistical measures\n",
    "def generate_data(mean, std_dev, min_value, max_value, percentiles, percentile_values, num_samples=2500):\n",
    "    # Generate initial data assuming a normal distribution\n",
    "    data = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n",
    "\n",
    "    # Define a function to adjust data to match specified percentiles\n",
    "    def adjust_percentiles(data, percentiles, percentile_values):\n",
    "        for i, pct in enumerate(percentiles):\n",
    "            desired_value = percentile_values[i]\n",
    "            current_value = np.percentile(data, pct)\n",
    "            adjustment = desired_value - current_value\n",
    "            data += adjustment * np.random.uniform(0.5, 1.5)  # Introduce variability with a random factor\n",
    "        return data\n",
    "\n",
    "    # Iteratively adjust data to match the specified percentiles\n",
    "    for _ in range(2500):  # Number of iterations\n",
    "        data = adjust_percentiles(data, percentiles, percentile_values)\n",
    "\n",
    "    # Clip the data to ensure it falls within the specified min and max range\n",
    "    data = np.clip(data, min_value, max_value)\n",
    "\n",
    "    # Rescale data to match the specified mean and standard deviation\n",
    "    data = (data - np.mean(data)) / np.std(data) * std_dev + mean\n",
    "\n",
    "    # Ensure all values are non-negative\n",
    "    data = np.clip(data, 0, None)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Given statistical measures for different variables\n",
    "variables = {\n",
    "    'BAL_ACCT_MIN_REQD': (2144, 3811, 0, 30000, [25, 50, 75], [1000, 2000, 2000]),\n",
    "    'N_TXN_AMT': (5362, 16947, 0, 995684, [25, 50, 75], [100, 600, 3000]),\n",
    "    'BAL_LAST_STMNT': (18055.8, 58088, 0.0, 3116780, [25, 50, 75], [0, 1240, 16115]),\n",
    "    'COUNT_TXN_PAST_3_DAYS_DEBIT': (73.2, 80, 0.0, 1275, [25, 50, 75], [29, 51, 89]),\n",
    "    'COUNT_TXN_PAST_4_DAYS_CREDIT': (655, 1312, 0.0, 10663, [25, 50, 75], [72, 154, 628]),\n",
    "    'SUM_TXN_PAST_6_DAYS_DEBIT': (1085580, 1022951, 0.0, 8943068, [25, 50, 75], [386282, 777815.8, 1451057]),\n",
    "    'SUM_ATM_TXN_PAST_6_DAYS': (138943, 142317, 100.0, 1140000, [25, 50, 75], [20000.0, 90000.0, 220500.0]),\n",
    "    'COUNT_ATM_TXN_PAST_7TO30DAYS': (2.947, 13.78, 0.0, 320.0, [25, 50, 75], [0.0, 0.0, 0.0]),\n",
    "    'COUNT_TXN_PAST_HOURS': (39.8, 104.3, 1.0, 1666.0, [25, 50, 75], [5.0, 10.0, 25.0]),\n",
    "    'SUM_TXN_PAST_HOURS': (69169, 109100, 1.0, 1194081, [25, 50, 75], [8000, 26009, 53000]),\n",
    "    'COUNT_TXN_PAST_HOURS_DEBIT': (3.10, 4.21, 0.0, 64, [25, 50, 75], [0.0, 2.0, 4.0]),\n",
    "    'SUM_TXN_PAST_HOURS_DEBIT': (44511.49, 62616, 1, 1194081, [25, 50, 75], [8000, 26009, 53000]),\n",
    "    'COUNT_TXN_PAST_HOURS_CREDIT': (36, 104, 0.0, 1662, [25, 50, 75], [2.0, 6.0, 22.0]),\n",
    "    'SUM_TXN_PAST_HOURS_CREDIT': (40416, 61516.99, 1.0, 1227024, [25, 50, 75], [7700, 21300, 48503]),\n",
    "    'DIGIT_SUM': (4.73, 4.8, 0.0, 46, [25, 50, 75], [1.0, 3.0, 6.0]),\n",
    "    'AVG_DIGIT_SUM': (4.6, 1.75, 1.0, 25.0, [25, 50, 75], [3.9, 4.63, 5.28]),\n",
    "    'NUM_DIGIT': (3.41, 1.065, 1.0, 6, [25, 50, 75], [3.0, 3.0, 4.0]),\n",
    "    'AVG_NUM_DIGIT': (3.41, 0.29, 1.0, 5.5, [25, 50, 75], [3.2, 3.4, 3.6]),\n",
    "    'AGE_OF_ACCT': (5.23, 20.64, 0.0, 368, [25, 50, 75], [1.0, 1.0, 2.0]),\n",
    "    'AGE_OF_CUSTOMER': (28, 11.3, 17.91, 224, [25, 50, 75], [21.58, 25.5, 32.166]),\n",
    "    '7D_1D_CR_1T_PARTIES': (586, 1174, 0.0, 8541, [25, 50, 75], [53, 106, 446]),\n",
    "    '14D_7D_CR_1T_PARTIES': (171, 708, 0.0, 8025, [25, 50, 75], [0.0, 5.0, 86.0]),\n",
    "    '7D_1D_CR_5T_PARTIES': (19.3, 37.9, 0.0, 328, [25, 50, 75], [2.0,7.0,18.0])\n",
    "}\n",
    "\n",
    "# Generate data for each variable and create a DataFrame\n",
    "data_dict = {}\n",
    "for var_name, params in variables.items():\n",
    "    mean, std_dev, min_value, max_value, percentiles, percentile_values = params\n",
    "    data_dict[var_name] = generate_data(mean, std_dev, min_value, max_value, percentiles, percentile_values)\n",
    "\n",
    "Mule_df_num = pd.DataFrame(data_dict)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the categories and their respective percentages for multiple variables\n",
    "# Define the categories and their respective percentages\n",
    "TYPE_OF_TXN = ['CREDIT', 'DEBIT']\n",
    "percentages_VAR1 = [0.7, 0.30]\n",
    "\n",
    "YR_OF_JOINING = ['2024', '2023', '2022','2021','2020']\n",
    "percentages_VAR2 = [0.95, 0.02, 0.01, 0.01, 0.01]\n",
    "\n",
    "CHEQ_ENABLED = ['N', 'Y']\n",
    "percentages_VAR3 = [0.53, 0.47]\n",
    "\n",
    "PASSBOOK = ['N', 'Y']\n",
    "percentages_VAR4 = [0.14, 0.86]\n",
    "\n",
    "BHIM_QR = ['N', 'Y']\n",
    "percentages_VAR5 = [0.92, 0.08]\n",
    "\n",
    "IB_REG = ['N', 'Y']\n",
    "percentages_VAR6 = [0.48, 0.52]\n",
    "\n",
    "DB_FLG = ['N', 'Y']\n",
    "percentages_VAR7 = [0.07, 0.93]\n",
    "\n",
    "VALID_MB = ['N', 'Y']\n",
    "percentages_VAR8 = [0.06, 0.94]\n",
    "\n",
    "AGE_OF_ACCOUNT= ['0', '1','2','3','4','5','6','7','REST']\n",
    "percentages_VAR9 = [0.16, 0.42,0.19,0.04,0.04,0.04,0.04,0.04,0.03]\n",
    "\n",
    "STATES=['Andhra Pradesh',\n",
    "'Arunachal Pradesh',\n",
    "'Assam',\n",
    "'Bihar',\n",
    "'Chhattisgarh',\n",
    "'Goa',\n",
    "'Gujarat',\n",
    "'Haryana',\n",
    "'Himachal Pradesh',\n",
    "'Jharkhand',\n",
    "'Karnataka',\n",
    "'Kerala',\n",
    "'Madhya Pradesh',\n",
    "'Maharashtra',\n",
    "'Manipur',\n",
    "'Meghalaya',\n",
    "'Mizoram',\n",
    "'Nagaland',\n",
    "'Odisha',\n",
    "'Punjab',\n",
    "'Rajasthan',\n",
    "'Sikkim',\n",
    "'Tamil Nadu',\n",
    "'Telangana',\n",
    "'Tripura',\n",
    "'Uttar Pradesh',\n",
    "'Uttarakhand',\n",
    "'West Bengal']\n",
    "percentages_VAR10 = [0.03,\n",
    "0.03,\n",
    "0.05,\n",
    "0.3,\n",
    "0.1,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.1,\n",
    "0.01,\n",
    "0.01,\n",
    "0.1,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0,\n",
    "0.1,\n",
    "0.01,\n",
    "0.01]\n",
    "\n",
    "# Number of samples to generate\n",
    "num_samples =2500\n",
    "\n",
    "# Generate the categorical data based on the given percentages for each variable\n",
    "data_var1 = np.random.choice(TYPE_OF_TXN, size=num_samples, p=percentages_VAR1)\n",
    "data_var2 = np.random.choice(YR_OF_JOINING, size=num_samples, p=percentages_VAR2)\n",
    "data_var3 = np.random.choice(CHEQ_ENABLED, size=num_samples, p=percentages_VAR3)\n",
    "data_var4 = np.random.choice(PASSBOOK, size=num_samples, p=percentages_VAR4)\n",
    "data_var5 = np.random.choice(BHIM_QR, size=num_samples, p=percentages_VAR5)\n",
    "data_var6 = np.random.choice(IB_REG, size=num_samples, p=percentages_VAR6)\n",
    "data_var7 = np.random.choice(DB_FLG, size=num_samples, p=percentages_VAR7)\n",
    "data_var8 = np.random.choice(VALID_MB, size=num_samples, p=percentages_VAR8)\n",
    "data_var9 = np.random.choice(AGE_OF_ACCOUNT, size=num_samples, p=percentages_VAR9)\n",
    "data_var10 = np.random.choice(STATES, size=num_samples, p=percentages_VAR10)\n",
    "\n",
    "# Create a DataFrame with the generated data\n",
    "Mule_df_cat = pd.DataFrame({\n",
    "    'TYPE_OF_TXN': data_var1,\n",
    "    'YR_OF_JOINING': data_var2,\n",
    "    'CHEQ_ENABLED': data_var3,\n",
    "    'PASSBOOK': data_var4,\n",
    "    'BHIM_QR': data_var5,\n",
    "    'IB_REG': data_var6,\n",
    "    'DB_FLG': data_var7,\n",
    "    'VALID_MB': data_var8,\n",
    "    'AGE_OF_ACCOUNT': data_var9,\n",
    "    'STATES': data_var10,\n",
    "})\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "Mule_merged_df = pd.concat([Mule_df_num, Mule_df_cat], axis=1)\n",
    "\n",
    "# Add the CUSTID column with values like C1, C2, C3, ...\n",
    "Mule_merged_df['CUSTID'] = ['C' + str(i) for i in range(1, len(Mule_merged_df) + 1)]\n",
    "Mule_merged_df['MULE_STATUS']=1\n",
    "# Reorder the columns to make CUSTID the first column\n",
    "cols = Mule_merged_df.columns.tolist()\n",
    "cols = ['CUSTID'] + ['MULE_STATUS']+[col for col in cols if col != 'CUSTID']\n",
    "Mule_merged_df = Mule_merged_df[cols]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Function to generate data with specified statistical measures\n",
    "def generate_data(mean, std_dev, min_value, max_value, percentiles, percentile_values, num_samples=2500):\n",
    "    # Generate initial data assuming a normal distribution\n",
    "    data = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n",
    "\n",
    "    # Define a function to adjust data to match specified percentiles\n",
    "    def adjust_percentiles(data, percentiles, percentile_values):\n",
    "        for i, pct in enumerate(percentiles):\n",
    "            desired_value = percentile_values[i]\n",
    "            current_value = np.percentile(data, pct)\n",
    "            adjustment = desired_value - current_value\n",
    "            data += adjustment * np.random.uniform(0.5, 1.5)  # Introduce variability with a random factor\n",
    "        return data\n",
    "\n",
    "    # Iteratively adjust data to match the specified percentiles\n",
    "    for _ in range(2500):  # Number of iterations\n",
    "        data = adjust_percentiles(data, percentiles, percentile_values)\n",
    "\n",
    "    # Clip the data to ensure it falls within the specified min and max range\n",
    "    data = np.clip(data, min_value, max_value)\n",
    "\n",
    "    # Rescale data to match the specified mean and standard deviation\n",
    "    data = (data - np.mean(data)) / np.std(data) * std_dev + mean\n",
    "\n",
    "    # Ensure all values are non-negative\n",
    "    data = np.clip(data, 0, None)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Given statistical measures for different variables\n",
    "variables = {\n",
    "    'BAL_ACCT_MIN_REQD': (3479, 6304, 0, 100000, [25, 50, 75], [500, 2000, 2000]),\n",
    "    'N_TXN_AMT': (21287.81, 2935629.0, 0, 1310500000, [25, 50, 75], [100, 500, 2000]),\n",
    "    'BAL_LAST_STMNT': (103339.15, 2751994.0, 0.0, 546822700, [25, 50, 75], [6.76, 1535.2, 12503.23]),\n",
    "    'COUNT_TXN_PAST_3_DAYS_DEBIT': (54.19, 99.0, 0.0, 2232, [25, 50, 75], [13.0, 25.0, 52.0]),\n",
    "    'COUNT_TXN_PAST_4_DAYS_CREDIT': (254.19, 949.0, 0.0, 16957, [25, 50, 75], [14.0, 44.0, 134.0]),\n",
    "    'SUM_TXN_PAST_6_DAYS_DEBIT': (202100.0, 519759.5, 0.94, 1310500000, [25, 50, 75], [54039.0, 162045.0, 528246.8]),\n",
    "    'SUM_ATM_TXN_PAST_6_DAYS': (44260.0, 49371.3, 100.0, 1120000.0, [25, 50, 75], [6000.0, 20000.0, 56000.0]),\n",
    "    'COUNT_ATM_TXN_PAST_7TO30DAYS': (3.99, 12.661, 0.0, 320.0, [25, 50, 75], [0.0, 1.0, 3.0]),\n",
    "    'COUNT_TXN_PAST_HOURS': (15.34, 96.64, 1.0, 3080.0, [25, 50, 75], [2.0, 4.0, 8.0]),\n",
    "    'SUM_TXN_PAST_HOURS': (64141.96, 3029704.0, 0.02, 1310500000, [25, 50, 75], [900.0, 5000.0, 21000.0]),\n",
    "    'COUNT_TXN_PAST_HOURS_DEBIT': (3.49, 17.54, 0.0, 792, [25, 50, 75], [0.0, 1.0, 3.0]),\n",
    "    'SUM_TXN_PAST_HOURS_DEBIT': (56452.86, 3639132.0, 0.02, 1310500000, [25, 50, 75], [500.0, 3936.0, 16000]),\n",
    "    'COUNT_TXN_PAST_HOURS_CREDIT': (11.49, 17.54, 0.0, 792, [25, 50, 75], [0.0, 1.0, 3.0]),\n",
    "    'SUM_TXN_PAST_HOURS_CREDIT': (36197.41, 723102.1, 1.0, 1350500000, [25, 50, 75], [1000.0, 4600.0, 16030]),\n",
    "    'DIGIT_SUM': (6.03, 5.11, 0.0, 57, [25, 50, 75], [2.0, 5.0, 8.0]),\n",
    "    'AVG_DIGIT_SUM': (5.9, 2.0, 1.0, 31.0, [25, 50, 75], [4.8, 5.59, 6.68]),\n",
    "    'NUM_DIGIT': (3.26, 1.023, 1.0, 10, [25, 50, 75], [3.0, 3.0, 4.0]),\n",
    "    'AVG_NUM_DIGIT': (3.26, 0.49, 1.0, 7.05, [25, 50, 75], [2.94, 3.23, 3.6]),\n",
    "    'AGE_OF_ACCT': (13.79, 39.80, 0.0, 1334, [25, 50, 75], [4.0, 5.0, 7.0]),\n",
    "    'AGE_OF_CUSTOMER': (32.72, 20.94, 3.33, 224.91, [25, 50, 75], [23.33, 29.0, 35.166]),\n",
    "    '7D_1D_CR_1T_PARTIES': (134.8, 342.17, 0.0, 1855, [25, 50, 75], [9.0, 28.0, 76.0]),\n",
    "    '14D_7D_CR_1T_PARTIES': (181.73, 558.46, 0.0, 3564.0, [25, 50, 75], [8.0, 26.0, 83.0]),\n",
    "    '7D_1D_CR_5T_PARTIES': (4.5, 9.18, 0.0, 88.0, [25, 50, 75], [0.0, 1.0, 5.0])\n",
    "}\n",
    "\n",
    "# Generate data for each variable and create a DataFrame\n",
    "data_dict = {}\n",
    "for var_name, params in variables.items():\n",
    "    mean, std_dev, min_value, max_value, percentiles, percentile_values = params\n",
    "    data_dict[var_name] = generate_data(mean, std_dev, min_value, max_value, percentiles, percentile_values)\n",
    "\n",
    "non_mule_df_num = pd.DataFrame(data_dict)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the categories and their respective percentages for multiple variables\n",
    "# Define the categories and their respective percentages\n",
    "TYPE_OF_TXN = ['CREDIT', 'DEBIT']\n",
    "percentages_VAR1 = [0.53, 0.47]\n",
    "\n",
    "YR_OF_JOINING = ['2024', '2023', '2022','2021','2020']\n",
    "percentages_VAR2 = [0.81, 0.1, 0.05, 0.03, 0.01]\n",
    "\n",
    "CHEQ_ENABLED = ['N', 'Y']\n",
    "percentages_VAR3 = [0.39, 0.61]\n",
    "\n",
    "PASSBOOK = ['N', 'Y']\n",
    "percentages_VAR4 = [0.2, 0.8]\n",
    "\n",
    "BHIM_QR = ['N', 'Y']\n",
    "percentages_VAR5 = [0.78, 0.22]\n",
    "\n",
    "IB_REG = ['N', 'Y']\n",
    "percentages_VAR6 = [0.73, 0.27]\n",
    "\n",
    "DB_FLG = ['N', 'Y']\n",
    "percentages_VAR7 = [0.04, 0.96]\n",
    "\n",
    "VALID_MB = ['N', 'Y']\n",
    "percentages_VAR8 = [0.02, 0.98]\n",
    "\n",
    "AGE_OF_ACCOUNT= ['0', '1','2','3','4','5','6','7','REST']\n",
    "percentages_VAR9 = [0.0, 0.03,0.04,0.07,0.16,0.22,0.20,0.10,0.18]\n",
    "\n",
    "STATES=['Andhra Pradesh',\n",
    "'Arunachal Pradesh',\n",
    "'Assam',\n",
    "'Bihar',\n",
    "'Chhattisgarh',\n",
    "'Goa',\n",
    "'Gujarat',\n",
    "'Haryana',\n",
    "'Himachal Pradesh',\n",
    "'Jharkhand',\n",
    "'Karnataka',\n",
    "'Kerala',\n",
    "'Madhya Pradesh',\n",
    "'Maharashtra',\n",
    "'Manipur',\n",
    "'Meghalaya',\n",
    "'Mizoram',\n",
    "'Nagaland',\n",
    "'Odisha',\n",
    "'Punjab',\n",
    "'Rajasthan',\n",
    "'Sikkim',\n",
    "'Tamil Nadu',\n",
    "'Telangana',\n",
    "'Tripura',\n",
    "'Uttar Pradesh',\n",
    "'Uttarakhand',\n",
    "'West Bengal']\n",
    "percentages_VAR10 = [0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.04,\n",
    "0.04,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.04,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.035,\n",
    "0.04,\n",
    "0.035,\n",
    "0.035]\n",
    "\n",
    "# Number of samples to generate\n",
    "num_samples =2500\n",
    "\n",
    "# Generate the categorical data based on the given percentages for each variable\n",
    "data_var1 = np.random.choice(TYPE_OF_TXN, size=num_samples, p=percentages_VAR1)\n",
    "data_var2 = np.random.choice(YR_OF_JOINING, size=num_samples, p=percentages_VAR2)\n",
    "data_var3 = np.random.choice(CHEQ_ENABLED, size=num_samples, p=percentages_VAR3)\n",
    "data_var4 = np.random.choice(PASSBOOK, size=num_samples, p=percentages_VAR4)\n",
    "data_var5 = np.random.choice(BHIM_QR, size=num_samples, p=percentages_VAR5)\n",
    "data_var6 = np.random.choice(IB_REG, size=num_samples, p=percentages_VAR6)\n",
    "data_var7 = np.random.choice(DB_FLG, size=num_samples, p=percentages_VAR7)\n",
    "data_var8 = np.random.choice(VALID_MB, size=num_samples, p=percentages_VAR8)\n",
    "data_var9 = np.random.choice(AGE_OF_ACCOUNT, size=num_samples, p=percentages_VAR9)\n",
    "data_var10 = np.random.choice(STATES, size=num_samples, p=percentages_VAR10)\n",
    "\n",
    "# Create a DataFrame with the generated data\n",
    "NonMule_df_cat = pd.DataFrame({\n",
    "    'TYPE_OF_TXN': data_var1,\n",
    "    'YR_OF_JOINING': data_var2,\n",
    "    'CHEQ_ENABLED': data_var3,\n",
    "    'PASSBOOK': data_var4,\n",
    "    'BHIM_QR': data_var5,\n",
    "    'IB_REG': data_var6,\n",
    "    'DB_FLG': data_var7,\n",
    "    'VALID_MB': data_var8,\n",
    "    'AGE_OF_ACCOUNT': data_var9,\n",
    "    'STATES': data_var10,\n",
    "})\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "NON_Mule_merged_df = pd.concat([non_mule_df_num, NonMule_df_cat], axis=1)\n",
    "\n",
    "# Add the CUSTID column with values like C1, C2, C3, ...\n",
    "NON_Mule_merged_df['CUSTID'] = ['CN' + str(i) for i in range(1, len(NON_Mule_merged_df) + 1)]\n",
    "NON_Mule_merged_df['MULE_STATUS'] =0\n",
    "# Reorder the columns to make CUSTID the first column\n",
    "cols = NON_Mule_merged_df.columns.tolist()\n",
    "cols = ['CUSTID'] +['MULE_STATUS']+ [col for col in cols if col != 'CUSTID']\n",
    "NON_Mule_merged_df = NON_Mule_merged_df[cols]\n",
    "\n",
    "# Concatenate the two DataFrames after resetting their index to avoid column name ambiguity\n",
    "merged_df = pd.concat([Mule_merged_df.reset_index(drop=True),\n",
    "                       NON_Mule_merged_df.reset_index(drop=True)],\n",
    "                      axis=0, ignore_index=False)\n",
    "\n",
    "\n",
    "# Shuffle the rows of the DataFrame\n",
    "merged_df = merged_df.sample(frac=1).reset_index(drop=True) # frac=1 means return all rows in random order\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the two DataFrames after resetting their index to avoid column name ambiguity\n",
    "merged_df = pd.concat([Mule_merged_df.reset_index(drop=True),\n",
    "                       NON_Mule_merged_df.reset_index(drop=True)],\n",
    "                      axis=0, ignore_index=False)\n",
    "\n",
    "\n",
    "# Shuffle the rows of the DataFrame\n",
    "merged_df = merged_df.sample(frac=1).reset_index(drop=True) # frac=1 means return all rows in random order\n",
    "\n",
    "\n",
    "X=merged_df.drop(['CUSTID','MULE_STATUS'],axis=1)\n",
    "y=merged_df.iloc[:,-1]      \n",
    "\n",
    "merged_df_encoded = pd.get_dummies(X, columns=['TYPE_OF_TXN', 'YR_OF_JOINING', 'CHEQ_ENABLED', 'PASSBOOK', 'BHIM_QR',\n",
    "       'IB_REG', 'DB_FLG', 'VALID_MB', 'AGE_OF_ACCOUNT', 'STATES'])\n",
    "# Assuming df_num is the DataFrame we created earlier\n",
    "# Split the dataset into train and test samples (80-20 split)\n",
    "#train_df, test_df = train_test_split(merged_df_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df_encoded, y, test_size=0.2, random_state=42)\n",
    "# train_df, test_df = train_test_split(merged_df, test_size=0.2, stratify=merged_df[target], random_state=42)\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f'{name} Model:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "\n",
    "# Train and evaluate models\n",
    "best_model = None\n",
    "best_f1_score = 0\n",
    "model_scores = {}\n",
    "best_model_instance = None\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    model_scores[model_name] = f1\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_model = model_name\n",
    "        best_model_instance = model\n",
    "\n",
    "# Print the best model and its F1 score\n",
    "print(f\"Best Model: {best_model} with F1 Score: {best_f1_score}\")\n",
    "\n",
    "# Save the best model details in a .yaml file\n",
    "model_details = {\n",
    "    'best_model': best_model,\n",
    "    'best_f1_score': best_f1_score,\n",
    "    'model_scores': model_scores\n",
    "}\n",
    "\n",
    "#import yaml\n",
    "import pickle\n",
    "\n",
    "#with open('best_ML_model_classification.yaml', 'w') as file:\n",
    "#    yaml.dump(model_details, file)\n",
    "\n",
    "# Save the best model as a pickle file\n",
    "with open('best_model_ML_Classification.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model_instance, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
